import os
import sys
import time
import json
import argparse
import requests
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from concurrent.futures import ThreadPoolExecutor

# è¨­å®š argparse ä¾†æ¥æ”¶ GitHub Actions åƒæ•¸
parser = argparse.ArgumentParser(description="Facebook Ads Image Scraper")
parser.add_argument("--json_file", required=True, help="Path to JSON file")
parser.add_argument("--output_dir", default="ads_media", help="Directory to save images")
args = parser.parse_args()

# è¨­å®š ChromeDriver è·¯å¾‘
CHROMEDRIVER_PATH = "/usr/bin/chromedriver"

# è®€å– JSON æª”æ¡ˆ
json_file = args.json_file
output_dir = args.output_dir

if not os.path.exists(json_file):
    print(f"âŒ JSON æª”æ¡ˆä¸å­˜åœ¨: {json_file}")
    sys.exit(1)

with open(json_file, 'r', encoding='utf-8') as f:
    data = json.load(f)

if len(data) == 0:
    print("âŒ JSON æª”æ¡ˆç‚ºç©º")

# è§£æ JSON
data_list = []
for entry in data:
    ad_id = entry.get("id", "")
    ad_title = entry.get("ad_creative_link_titles", [""])[0] if "ad_creative_link_titles" in entry else ""
    ad_text = entry.get("ad_creative_bodies", [""])[0] if "ad_creative_bodies" in entry else ""
    ad_image_url = entry.get("ad_snapshot_url", "")

    data_list.append({
        "ad_id": ad_id,
        "ad_title": ad_title,
        "ad_text": ad_text,
        "ad_image_url": ad_image_url
    })

df_ads = pd.DataFrame(data_list)

# è¨­å®š Selenium ç€è¦½å™¨
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

service = Service(CHROMEDRIVER_PATH)
browser = webdriver.Chrome(service=service, options=chrome_options)

# ä¸‹è¼‰åœ–ç‰‡å‡½æ•¸
def download_image(url, folder_name, file_name):
    try:
        response = requests.get(url, stream=True)
        if response.status_code == 200:
            os.makedirs(folder_name, exist_ok=True)
            file_path = os.path.join(folder_name, file_name)
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(1024):
                    f.write(chunk)
            print(f"âœ… ä¸‹è¼‰æˆåŠŸ: {file_path}")
            return file_path
        else:
            print(f"âŒ ç„¡æ³•ä¸‹è¼‰: {url}")
            return None
    except Exception as e:
        print(f"âš ï¸ ä¸‹è¼‰éŒ¯èª¤: {e}")
        return None

# æ“·å– Facebook å»£å‘Šåœ–ç‰‡
def scrape_facebook_ad(ad_url, folder_name, ad_id):
    browser.get(ad_url)
    time.sleep(5)

    soup = BeautifulSoup(browser.page_source, "html.parser")

    # æ‰¾åˆ°å»£å‘Šå…§å®¹çš„ class
    ad_container = soup.find("div", class_="_8n-d")
    if not ad_container:
        print(f"âš ï¸ å»£å‘Š {ad_id} æ²’æœ‰æ‰¾åˆ°å…§å®¹")
        return None

    # å–å¾—æ‰€æœ‰åœ–ç‰‡
    images = ad_container.find_all("img")
    ad_folder = os.path.join(folder_name, ad_id)  # æ¯å€‹å»£å‘Š ID å»ºç«‹ç¨ç«‹è³‡æ–™å¤¾
    os.makedirs(ad_folder, exist_ok=True)

    for i, img in enumerate(images):
        img_url = img["src"]
        file_name = f"image_{i}.jpg"
        download_image(img_url, ad_folder, file_name)

    print(f"ğŸ¯ å»£å‘Š {ad_id} ä¸‹è¼‰å®Œæˆï¼")

# ä½¿ç”¨å¤šç·šç¨‹è™•ç†æ‰€æœ‰å»£å‘Š
def process_ad(row):
    if row["ad_image_url"]:
        scrape_facebook_ad(row["ad_image_url"], output_dir, row["ad_id"])

with ThreadPoolExecutor(max_workers=5) as executor:  # æœ€å¤š 5 æ¢ç·šç¨‹
    executor.map(process_ad, [row for _, row in df_ads.iterrows()])

browser.quit()
